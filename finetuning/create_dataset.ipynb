{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = '''You are a specialist in creating databases for LLMs in JSON format with the fields \"instruction\", \"input\", and \"output\".\n",
    "Extract important information from large amounts of text to create the entries for \"instruction\", \"input\", and \"output\" for the Dataset.\n",
    "Use the text provided in the \"Text input\" section.\n",
    "Example provided for guidance.\n",
    "\n",
    "Example:\n",
    "\n",
    "Text input:\n",
    "Magic in George R. R. Martin's world manifests in different ways. In Westeros, visible magic has largely waned, leaving behind traces such as the Children of the Forest and their bond with weirwood trees. Conversely, in Essos, magic is more widespread. There are sorcerers in Qarth, the warlocks of Asshai, the Red Priests of R'hllor who wield shadow magic, and the glass candles of the Citadel with mysterious properties.\n",
    "House Stark is a prominent noble house in Westeros, governing the North from Winterfell. Recognized by their direwolf sigil and their motto 'Winter is Coming,' the Starks are celebrated for their honor, resilience, and ties to the Old Gods. Throughout A Song of Ice and Fire, Starks like Eddard 'Ned' Stark, his wife Catelyn Tully, and their children are central figures in the War of the Five Kings and the looming threat beyond the Wall.\n",
    "\n",
    "Text output:\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Write a short biography of a famous house in Westeros.\",\n",
    "        \"input\": \"House Stark\",\n",
    "        \"output\": \"House Stark is a noble house in Westeros, ruling the North from Winterfell. Known for their sigil of the direwolf and their motto 'Winter is Coming,' the Starks are renowned for their honor, resilience, and connection to the Old Gods. Throughout A Song of Ice and Fire, Starks like Eddard 'Ned' Stark, his wife Catelyn Tully, and their children play pivotal roles in the War of the Five Kings and the greater threat beyond the Wall.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of magic in Westeros and Essos.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Magic in George R. R. Martin's world exists in various forms. Westeros has largely seen a decline in overt magic, with remnants like the Children of the Forest and their connection to weirwood trees. In Essos, however, magic is more prevalent. There are sorcerers in Qarth, the warlocks of Asshai, the Red Priests of R'hllor who practice shadow magic, and the glass candles of the Citadel with unknown properties.\"\n",
    "    }\n",
    "    ... to be continued\n",
    "]\n",
    "\n",
    "Additional Guidelines:\n",
    "Describe only the data listed in the text, use the \"Text output\" format (JSON), and consider the examples only as a reference to extract relevant information about the subjects mentioned in the \"text input\" text.\n",
    "You will receive user input data in diferent languages, outputs should be in input's language.\n",
    "Use UTF-8 for generated text.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the OpenAI library and sets the API key\n",
    "from openai import AzureOpenAI\n",
    "from unidecode import unidecode\n",
    "import json_repair\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "\n",
    "# Configuração da API da OpenAI\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://saulopenai.openai.azure.com/\",\n",
    "    api_key='c6e83b7e47254fa68dddcee008fe8aeb',\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "# Função para extrair texto do PDF\n",
    "def extract_text_from_pdf(pdf_path, start_page=5, end_page=None):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "    if end_page is None:\n",
    "        end_page = pdf_document.page_count\n",
    "    for page_num in range(start_page, min(end_page, pdf_document.page_count)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        extracted_text += text\n",
    "    pdf_document.close()\n",
    "    return extracted_text\n",
    "\n",
    "# Função para dividir o texto em blocos de 500 tokens\n",
    "def split_text_into_chunks(text, model_name=\"gpt-4o\", chunk_size=500):\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    text_chunks = [enc.decode(chunk) for chunk in chunks]\n",
    "    return text_chunks\n",
    "# Prompt\n",
    "prompt = '''You are a specialist in creating databases for LLMs in JSON format with the fields \"instruction\", \"input\", and \"output\".\n",
    "Extract important information from large amounts of text to create the entries for \"instruction\", \"input\", and \"output\" for the Dataset.\n",
    "Use the text provided in the \"Text input\" section.\n",
    "Example provided for guidance.\n",
    "\n",
    "Example:\n",
    "\n",
    "Text input:\n",
    "Magic in George R. R. Martin's world manifests in different ways. In Westeros, visible magic has largely waned, leaving behind traces such as the Children of the Forest and their bond with weirwood trees. Conversely, in Essos, magic is more widespread. There are sorcerers in Qarth, the warlocks of Asshai, the Red Priests of R'hllor who wield shadow magic, and the glass candles of the Citadel with mysterious properties.\n",
    "House Stark is a prominent noble house in Westeros, governing the North from Winterfell. Recognized by their direwolf sigil and their motto 'Winter is Coming,' the Starks are celebrated for their honor, resilience, and ties to the Old Gods. Throughout A Song of Ice and Fire, Starks like Eddard 'Ned' Stark, his wife Catelyn Tully, and their children are central figures in the War of the Five Kings and the looming threat beyond the Wall.\n",
    "\n",
    "Text output:\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Write a short biography of a famous house in Westeros.\",\n",
    "        \"input\": \"House Stark\",\n",
    "        \"output\": \"House Stark is a noble house in Westeros, ruling the North from Winterfell. Known for their sigil of the direwolf and their motto 'Winter is Coming,' the Starks are renowned for their honor, resilience, and connection to the Old Gods. Throughout A Song of Ice and Fire, Starks like Eddard 'Ned' Stark, his wife Catelyn Tully, and their children play pivotal roles in the War of the Five Kings and the greater threat beyond the Wall.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of magic in Westeros and Essos.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Magic in George R. R. Martin's world exists in various forms. Westeros has largely seen a decline in overt magic, with remnants like the Children of the Forest and their connection to weirwood trees. In Essos, however, magic is more prevalent. There are sorcerers in Qarth, the warlocks of Asshai, the Red Priests of R'hllor who practice shadow magic, and the glass candles of the Citadel with unknown properties.\"\n",
    "    }\n",
    "    ... to be continued\n",
    "]\n",
    "\n",
    "Additional Guidelines:\n",
    "Describe only the data listed in the text, use the \"Text output\" format (JSON), and consider the examples only as a reference to extract relevant information about the subjects mentioned in the \"text input\" text.\n",
    "You will receive user input data in diferent languages, outputs should be in input's language.\n",
    "Use UTF-8 for generated text.\n",
    "'''\n",
    "# Função para enviar os blocos de texto para a API e gerar JSON\n",
    "def process_chunks_with_api(text_chunks, model_name=\"gpt-4o\", prompt_start=prompt):\n",
    "    response_content = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_start},\n",
    "                {\"role\": \"user\", \"content\": chunk}\n",
    "            ]\n",
    "        )\n",
    "        response_text = response.choices[0].message.content\n",
    "        try:\n",
    "            response_json = json.loads(response_text)\n",
    "            response_content.extend(response_json)\n",
    "        except json.JSONDecodeError:\n",
    "            repaired_text = json_repair.repair(response_text)\n",
    "            try:\n",
    "                response_json = json.loads(repaired_text)\n",
    "                response_content.extend(response_json)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Failed to parse response as JSON, skipping chunk.\")\n",
    "    \n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Conting tokens\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model_name=\"gpt-4o\"):\n",
    "    # Carregar o codificador para o modelo especificado\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    \n",
    "    # Codificar o texto em tokens\n",
    "    tokens = enc.encode(text)\n",
    "    \n",
    "    # Retornar a contagem de tokens\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "In the world of fantasy literature, few authors have created as intricate and immersive a universe as J.R.R. Tolkien. His works, most notably \"The Hobbit\" and \"The Lord of the Rings,\" have become cornerstones of modern fantasy, setting the bar for world-building, character development, and epic storytelling. The richness of Middle-earth, with its diverse cultures, languages, and histories, continues to captivate readers and inspire writers decades after its creation.\n",
    "\n",
    "Tolkien's Middle-earth is not just a backdrop for his stories; it is a living, breathing world with its own geography, politics, and social structures. The Shire, home to the hobbits, is a pastoral idyll, a symbol of peace and simplicity. In contrast, the dark lands of Mordor, with Mount Doom at its heart, represent the ultimate evil and corruption. Each location in Middle-earth is meticulously crafted, with its own unique atmosphere and significance.\n",
    "\n",
    "The characters in Tolkien's works are equally memorable and well-developed. Frodo Baggins, the unlikely hero, embodies the themes of courage and sacrifice. His journey from the Shire to Mount Doom is a testament to the resilience of the human (or hobbit) spirit. Alongside Frodo, characters like Aragorn, Legolas, and Gimli add depth and variety to the narrative, each bringing their own strengths and perspectives to the quest to destroy the One Ring.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# The deployment name you chose when you deployed the GPT-3.5-Turbo or GPT-4 model.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[0;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: text}\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:640\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    638\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    639\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    642\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    643\u001b[0m             {\n\u001b[0;32m    644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    645\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    646\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    647\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    648\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    667\u001b[0m             },\n\u001b[0;32m    668\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    669\u001b[0m         ),\n\u001b[0;32m    670\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    671\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    672\u001b[0m         ),\n\u001b[0;32m    673\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    674\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    676\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1250\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1238\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1247\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1248\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1249\u001b[0m     )\n\u001b[1;32m-> 1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    932\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    933\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    934\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    935\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    936\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    937\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1014\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1016\u001b[0m         options,\n\u001b[0;32m   1017\u001b[0m         cast_to,\n\u001b[0;32m   1018\u001b[0m         retries,\n\u001b[0;32m   1019\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1020\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1021\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1022\u001b[0m     )\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1064\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1065\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1066\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1067\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1068\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1069\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1014\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1016\u001b[0m         options,\n\u001b[0;32m   1017\u001b[0m         cast_to,\n\u001b[0;32m   1018\u001b[0m         retries,\n\u001b[0;32m   1019\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1020\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1021\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1022\u001b[0m     )\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1064\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1065\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1066\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1067\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1068\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1069\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\saulo.leite\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1030\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1029\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1033\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1034\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1038\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # The deployment name you chose when you deployed the GPT-3.5-Turbo or GPT-4 model.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "pdf_path = \"gram_tupi.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Dividir o texto em blocos de 500 tokens\n",
    "text_chunks = split_text_into_chunks(text)\n",
    "\n",
    "# Processar os blocos com a API e salvar as respostas em JSON\n",
    "responses = process_chunks_with_api(text_chunks, prompt_start=prompt)\n",
    "json_data = json_repair.loads(responses.strip('```').strip('json').strip())\n",
    "\n",
    "# Salvar as respostas em um arquivo JSON\n",
    "with open('dataset.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Processamento completo. Respostas salvas em 'responses.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de chamadas: 14\n",
      "Chunks processados: 1\n",
      "Chunks processados: 2\n",
      "Chunks processados: 3\n",
      "Chunks processados: 4\n",
      "Chunks processados: 5\n",
      "Chunks processados: 6\n",
      "Chunks processados: 7\n",
      "Chunks processados: 8\n",
      "Chunks processados: 9\n",
      "Chunks processados: 10\n",
      "Chunks processados: 11\n",
      "Chunks processados: 12\n",
      "Chunks processados: 13\n",
      "Chunks processados: 14\n",
      "Processamento completo. Respostas salvas em 'responses.json'.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from unidecode import unidecode\n",
    "import json_repair\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "\n",
    "# Configuração da API da OpenAI\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"\",\n",
    "    api_key='',\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "# Função para extrair texto do PDF\n",
    "def extract_text_from_pdf(pdf_path, start_page=0, end_page=None):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "    if end_page is None:\n",
    "        end_page = pdf_document.page_count\n",
    "    for page_num in range(start_page, min(end_page, pdf_document.page_count)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        extracted_text += text\n",
    "    pdf_document.close()\n",
    "    return extracted_text\n",
    "\n",
    "# Função para dividir o texto em blocos de tokens\n",
    "def split_text_into_chunks(text, model_name=\"gpt-4o\", chunk_size=500):\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    text_chunks = [enc.decode(chunk) for chunk in chunks]\n",
    "    print(\"Quantidade de chamadas:\", len(text_chunks))\n",
    "    return text_chunks\n",
    "\n",
    "# Prompt\n",
    "prompt = '''You are a specialist in creating databases for LLMs in JSON format with the fields \"instruction\", \"input\", and \"output\".\n",
    "Extract important information from large amounts of text to create the entries for \"instruction\", \"input\", and \"output\" for the Dataset.\n",
    "Use the text provided in the \"Text input\" section.\n",
    "Example provided for guidance.\n",
    "\n",
    "Example:\n",
    "\n",
    "Text input:\n",
    "Magic in George R. R. Martin's world manifests in different ways. In Westeros, visible magic has largely waned, leaving behind traces such as the Children of the Forest and their bond with weirwood trees. Conversely, in Essos, magic is more widespread. There are sorcerers in Qarth, the warlocks of Asshai, the Red Priests of R'hllor who wield shadow magic, and the glass candles of the Citadel with mysterious properties.\n",
    "House Stark is a prominent noble house in Westeros, governing the North from Winterfell. Recognized by their direwolf sigil and their motto 'Winter is Coming,' the Starks are celebrated for their honor, resilience, and ties to the Old Gods. Throughout A Song of Ice and Fire, Starks like Eddard 'Ned' Stark, his wife Catelyn Tully, and their children are central figures in the War of the Five Kings and the looming threat beyond the Wall.\n",
    "\n",
    "Text output:\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Write a short biography of a famous house in Westeros.\",\n",
    "        \"input\": \"House Stark\",\n",
    "        \"output\": \"House Stark is a noble house in Westeros, ruling the North from Winterfell. Known for their sigil of the direwolf and their motto 'Winter is Coming,' the Starks are renowned for their honor, resilience, and connection to the Old Gods. Throughout A Song of Ice and Fire, Starks like Eddard 'Ned' Stark, his wife Catelyn Tully, and their children play pivotal roles in the War of the Five Kings and the greater threat beyond the Wall.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of magic in Westeros and Essos.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Magic in George R. R. Martin's world exists in various forms. Westeros has largely seen a decline in overt magic, with remnants like the Children of the Forest and their connection to weirwood trees. In Essos, however, magic is more prevalent. There are sorcerers in Qarth, the warlocks of Asshai, the Red Priests of R'hllor who practice shadow magic, and the glass candles of the Citadel with unknown properties.\"\n",
    "    }\n",
    "    ... to be continued\n",
    "]\n",
    "\n",
    "Additional Guidelines:\n",
    "Describe only the data listed in the text, use the \"Text output\" format (JSON), and consider the examples only as a reference to extract relevant information about the subjects mentioned in the \"text input\" text.\n",
    "You will receive user input data in diferent languages, outputs should be in input's language.\n",
    "Use UTF-8 for generated text.\n",
    "'''\n",
    "\n",
    "# Função para enviar os blocos de texto para a API e gerar JSON\n",
    "def process_chunks_with_api(text_chunks, model_name=\"gpt4o-compass-dev\", prompt_start=prompt):\n",
    "    response_content = []\n",
    "    counter = 0  # Inicializa o contador\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_start},\n",
    "                {\"role\": \"user\", \"content\": chunk}\n",
    "            ]\n",
    "        )\n",
    "        response_text = response.choices[0].message.content\n",
    "        try:\n",
    "            response_json = json_repair.loads(response_text)\n",
    "            response_content.extend(response_json)\n",
    "        except json.JSONDecodeError:\n",
    "            repaired_text = json_repair.repair(response_text)\n",
    "            try:\n",
    "                response_json = json.loads(repaired_text)\n",
    "                response_content.extend(response_json)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Failed to parse response as JSON, skipping chunk.\")\n",
    "        \n",
    "        counter += 1  # Incrementa o contador\n",
    "        print(f\"Chunks processados: {counter}\")  # Imprime o contador\n",
    "    \n",
    "    return response_content\n",
    "\n",
    "# Caminho para o PDF\n",
    "pdf_path = \"gloss.pdf\"\n",
    "\n",
    "# Extraindo texto do PDF\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Dividindo o texto em blocos de x tokens\n",
    "text_chunks = split_text_into_chunks(extracted_text)\n",
    "\n",
    "# Processando os blocos de texto com a API\n",
    "response_content = process_chunks_with_api(text_chunks)\n",
    "\n",
    "\n",
    "with open('dataset3.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(response_content, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Processar os blocos com a API e salvar as respostas em JSON\n",
    "\n",
    "print(\"Processamento completo. Respostas salvas em 'responses.json'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
